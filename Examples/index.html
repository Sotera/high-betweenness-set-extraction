---
layout: base
---
<h3><a name="examples" class="anchor" href="#examples"><span class="octicon octicon-link"></span></a>Example</h3>

<p>A small example is included to verify installation and the general concept in the 'example' directory.</p>
<ol>
    <li>Move the job jar to your current directory for use and create a run configuration for your cluster.
<pre>
    <code>
        $ cp build/libs/HighBetweennessSetExtraction-1.0.jar .
        $ cp run-template.sh run-example.sh
    </code>
</pre>
    </li>
    <li>
        Edit run-example.sh for your clusrer configuration. You must set the options for:
        <ul>
            <li>
                HDFS_HOST
            </li>
            <li>
                ZK_LIST (zoo keeper nodes)
            </li>
        </ul>
    </li>
    <li>
        Now you'll need to put the example data set int hdfs at the location you specified for EDGE_INPUT_PATH (you can change this or use the default), also see the value for
        OUTPUT_DIR.
<pre>
    <code>
        $ hadoop fs -put example/example.csv /tmp/hbse_example/input/example.csv
    </code>
</pre>
    </li>
    <li>
        Finally execute the example and view the results in HDFS.
<pre>
    <code>
        $ ./run-example.sh
    </code>
</pre>
    </li>
</ol>
<h3><a name="expected-results" class="anchor" href="#expected-results"><span class="octicon octicon-link"></span></a>Expected Example Results</h3>
<p>
    The output directory in hdfs should have three entries:
</p>
<ul>
    <li>
        final_set.csv - the extracted set of vertices believed to have the highest betweenness.
    </li>
    <li>
        stats.csv - Some information about the run
    </li>
    <li>
        values - HDFS directory containing the approximated betweenness value for all vertices
    </li>
</ul>
<h3><a name="expected-results-img" class="anchor" href="#expected-results-img"><span class="octicon octicon-link"></span></a>Expected Example Results Image</h3>
<img src="/high-betweenness-set-extraction/img/hbse_example_image.png"/>

<h3><a name="own-data" class="anchor" href="#own-data"><span class="octicon octicon-link"></span></a>Running On Your Own Data</h3>
<p>After running the example above you are ready to try out your own data. </p>
<ol>
    <li>Make another copy of the run script.
<pre>
    <code>
        $ cp run-example.sh run-mydata.sh
    </code>
</pre>
    </li>
    <li>
        Edit run-mydata.sh
        <ul>
            <li>
                VERTEX_COUNT must be set to number of nodes in your data
            </li>
            <li>
                OUPUT_DIR - point to your output location in hdfs
            </li>
            <li>
                EDGE_INPUT_DIR - point to you edge list in hdfs
            </li>
        </ul>
    </li>
</ol>
<p>You may also want to adjust various parameters in the VARIABLE JOB SETTINGS section, such as the pivot batch size, result set size, stability cutoff, or stability counter to
    taylor the job for your specific application/data.</p>